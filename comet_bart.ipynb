{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5XJSb4ZODcGe",
        "jkzV9CVRES7J",
        "AuJvPdg2EBuY",
        "YqV2bufsEINi"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installations"
      ],
      "metadata": {
        "id": "bMYU7wDQDino"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r \"/content/requirements.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiE5hhe0Jh7f",
        "outputId": "41f55e1a-3aef-41d8-a330-f65ccec45c2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 1)) (2.15.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 2)) (1.2.2)\n",
            "Collecting seqeval (from -r /content/requirements.txt (line 3))\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 4)) (5.9.5)\n",
            "Collecting sacrebleu (from -r /content/requirements.txt (line 5))\n",
            "  Downloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge-score (from -r /content/requirements.txt (line 6))\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 7)) (4.9.4)\n",
            "Collecting pytorch-lightning==0.8.1 (from -r /content/requirements.txt (line 8))\n",
            "  Downloading pytorch_lightning-0.8.1-py3-none-any.whl (293 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.1/293.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 9)) (3.7.1)\n",
            "Collecting git-python==1.0.3 (from -r /content/requirements.txt (line 10))\n",
            "  Downloading git_python-1.0.3-py2.py3-none-any.whl (1.9 kB)\n",
            "Collecting streamlit (from -r /content/requirements.txt (line 11))\n",
            "  Downloading streamlit-1.31.0-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting elasticsearch (from -r /content/requirements.txt (line 12))\n",
            "  Downloading elasticsearch-8.12.0-py3-none-any.whl (431 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.9/431.9 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 13)) (1.5.3)\n",
            "Collecting nlp (from -r /content/requirements.txt (line 14))\n",
            "  Downloading nlp-0.4.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 15)) (3.7.2)\n",
            "Collecting transformers==3.0.2 (from -r /content/requirements.txt (line 16))\n",
            "  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.0/769.0 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Ignored the following versions that require a different python version: 0.55.2 Requires-Python <3.5\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.5.1 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.5.1\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yGKxAQ4TQOy",
        "outputId": "a2490fa9-a108-4461-b490-a5f9c0e629aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Using cached rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.23.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=7b50491b5028f839ee525eaec0f7edfd9bfe511ca868668ffe6bf8dc00fd93d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHyMDtTrTQbi",
        "outputId": "bcc3589c-39e4-44ac-877d-f75148549f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Using cached sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.12.25)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.23.5)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-2.8.2 sacrebleu-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Navigate to the directory where your script is located\n",
        "%cd \"/content/download_model.sh\"\n",
        "\n",
        "# Grant execute permission to the script\n",
        "!chmod +x \"/content/download_model.sh\"\n",
        "\n",
        "# Run the script\n",
        "!\"/content/download_model.sh\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2unElye6THOU",
        "outputId": "7f4aa591-2509-449a-8d31-56d25180f2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 20] Not a directory: '/content/download_model.sh'\n",
            "/content\n",
            "--2024-02-07 16:38:59--  https://storage.googleapis.com/ai2-mosaic-public/projects/mosaic-kgs/comet-atomic_2020_BART.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.196.207, 74.125.134.207, 74.125.139.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.196.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1507095346 (1.4G) [application/zip]\n",
            "Saving to: ‘comet-atomic_2020_BART.zip’\n",
            "\n",
            "comet-atomic_2020_B 100%[===================>]   1.40G  74.3MB/s    in 17s     \n",
            "\n",
            "2024-02-07 16:39:16 (86.2 MB/s) - ‘comet-atomic_2020_BART.zip’ saved [1507095346/1507095346]\n",
            "\n",
            "Archive:  comet-atomic_2020_BART.zip\n",
            "   creating: comet-atomic_2020_BART/\n",
            "  inflating: comet-atomic_2020_BART/added_tokens.json  \n",
            "  inflating: comet-atomic_2020_BART/.DS_Store  \n",
            "  inflating: __MACOSX/comet-atomic_2020_BART/._.DS_Store  \n",
            "  inflating: comet-atomic_2020_BART/tokenizer_config.json  \n",
            "  inflating: comet-atomic_2020_BART/special_tokens_map.json  \n",
            "  inflating: comet-atomic_2020_BART/config.json  \n",
            "  inflating: comet-atomic_2020_BART/.added_tokens.json.swp  \n",
            "  inflating: comet-atomic_2020_BART/merges.txt  \n",
            "  inflating: comet-atomic_2020_BART/pytorch_model.bin  \n",
            "  inflating: comet-atomic_2020_BART/vocab.json  \n",
            "--2024-02-07 16:39:49--  https://storage.googleapis.com/ai2-mosaic-public/projects/mosaic-kgs/comet-atomic_2020_BART_aaai.tar.gz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.203.207, 172.253.123.207, 142.250.97.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.203.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1505937205 (1.4G) [application/x-tar]\n",
            "Saving to: ‘comet-atomic_2020_BART_aaai.tar.gz’\n",
            "\n",
            "comet-atomic_2020_B 100%[===================>]   1.40G   100MB/s    in 16s     \n",
            "\n",
            "2024-02-07 16:40:06 (87.4 MB/s) - ‘comet-atomic_2020_BART_aaai.tar.gz’ saved [1505937205/1505937205]\n",
            "\n",
            "comet-atomic_2020_BART_aaai/\n",
            "comet-atomic_2020_BART_aaai/added_tokens.json\n",
            "comet-atomic_2020_BART_aaai/tokenizer_config.json\n",
            "comet-atomic_2020_BART_aaai/special_tokens_map.json\n",
            "comet-atomic_2020_BART_aaai/merges.txt\n",
            "comet-atomic_2020_BART_aaai/pytorch_model.bin\n",
            "comet-atomic_2020_BART_aaai/vocab.json\n",
            "comet-atomic_2020_BART_aaai/config.json.org\n",
            "comet-atomic_2020_BART_aaai/config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lighting base"
      ],
      "metadata": {
        "id": "5XJSb4ZODcGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.utilities import rank_zero_info\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    AutoModel,\n",
        "    AutoModelForPreTraining,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForTokenClassification,\n",
        "    AutoModelWithLMHead,\n",
        "    AutoTokenizer,\n",
        "    PretrainedConfig,\n",
        "    PreTrainedTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "MODEL_MODES = {\n",
        "    \"base\": AutoModel,\n",
        "    \"sequence-classification\": AutoModelForSequenceClassification,\n",
        "    \"question-answering\": AutoModelForQuestionAnswering,\n",
        "    \"pretraining\": AutoModelForPreTraining,\n",
        "    \"token-classification\": AutoModelForTokenClassification,\n",
        "    \"language-modeling\": AutoModelWithLMHead,\n",
        "    \"summarization\": AutoModelForSeq2SeqLM,\n",
        "    \"translation\": AutoModelForSeq2SeqLM,\n",
        "}\n",
        "\n",
        "\n",
        "class BaseTransformer(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hparams: argparse.Namespace,\n",
        "        num_labels=None,\n",
        "        mode=\"base\",\n",
        "        config=None,\n",
        "        tokenizer=None,\n",
        "        model=None,\n",
        "        **config_kwargs\n",
        "    ):\n",
        "        \"\"\"Initialize a model, tokenizer and config.\"\"\"\n",
        "        super().__init__()\n",
        "        # TODO: move to self.save_hyperparameters()\n",
        "        # self.save_hyperparameters()\n",
        "        # can also expand arguments into trainer signature for easier reading\n",
        "\n",
        "        self.hparams = hparams\n",
        "        self.step_count = 0\n",
        "        self.tfmr_ckpts = {}\n",
        "        self.output_dir = Path(self.hparams.output_dir)\n",
        "        cache_dir = self.hparams.cache_dir if self.hparams.cache_dir else None\n",
        "        if config is None:\n",
        "            self.config = AutoConfig.from_pretrained(\n",
        "                self.hparams.config_name if self.hparams.config_name else self.hparams.model_name_or_path,\n",
        "                **({\"num_labels\": num_labels} if num_labels is not None else {}),\n",
        "                cache_dir=cache_dir,\n",
        "                **config_kwargs,\n",
        "            )\n",
        "        else:\n",
        "            self.config: PretrainedConfig = config\n",
        "        if tokenizer is None:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                self.hparams.tokenizer_name if self.hparams.tokenizer_name else self.hparams.model_name_or_path,\n",
        "                cache_dir=cache_dir,\n",
        "            )\n",
        "        else:\n",
        "            self.tokenizer: PreTrainedTokenizer = tokenizer\n",
        "        self.model_type = MODEL_MODES[mode]\n",
        "        if model is None:\n",
        "            self.model = self.model_type.from_pretrained(\n",
        "                self.hparams.model_name_or_path,\n",
        "                from_tf=bool(\".ckpt\" in self.hparams.model_name_or_path),\n",
        "                config=self.config,\n",
        "                cache_dir=cache_dir,\n",
        "            )\n",
        "        else:\n",
        "            self.model = model\n",
        "\n",
        "    def load_hf_checkpoint(self, *args, **kwargs):\n",
        "        self.model = self.model_type.from_pretrained(*args, **kwargs)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
        "        model = self.model\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": self.hparams.weight_decay,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
        "        self.opt = optimizer\n",
        "\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=self.total_steps\n",
        "        )\n",
        "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def test_step(self, batch, batch_nb):\n",
        "        return self.validation_step(batch, batch_nb)\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        return self.validation_end(outputs)\n",
        "\n",
        "    def setup(self, step):\n",
        "        train_batch_size = self.hparams.train_batch_size\n",
        "        dataloader = self.get_dataloader(\"train\", train_batch_size)\n",
        "        self.train_loader = dataloader\n",
        "        self.total_steps = (\n",
        "            (len(dataloader.dataset) // (train_batch_size * max(1, self.hparams.gpus)))\n",
        "            // self.hparams.accumulate_grad_batches\n",
        "            * float(self.hparams.max_epochs)\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return self.train_loader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return self.get_dataloader(\"dev\", self.hparams.eval_batch_size)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return self.get_dataloader(\"test\", self.hparams.eval_batch_size)\n",
        "\n",
        "    def _feature_file(self, mode):\n",
        "        return os.path.join(\n",
        "            self.hparams.data_dir,\n",
        "            \"cached_{}_{}_{}\".format(\n",
        "                mode,\n",
        "                list(filter(None, self.hparams.model_name_or_path.split(\"/\"))).pop(),\n",
        "                str(self.hparams.max_seq_length),\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    @pl.utilities.rank_zero_only\n",
        "    def on_save_checkpoint(self, checkpoint: Dict[str, Any]) -> None:\n",
        "        save_path = self.output_dir.joinpath(\"best_tfmr\")\n",
        "        save_path.mkdir(exist_ok=True)\n",
        "        self.model.config.save_step = self.step_count\n",
        "        self.model.save_pretrained(save_path)\n",
        "        self.tokenizer.save_pretrained(save_path)\n",
        "        self.tfmr_ckpts[self.step_count] = save_path\n",
        "\n",
        "    @staticmethod\n",
        "    def add_model_specific_args(parser, root_dir):\n",
        "        parser.add_argument(\n",
        "            \"--model_name_or_path\",\n",
        "            default=None,\n",
        "            type=str,\n",
        "            required=True,\n",
        "            help=\"Path to pretrained model or model identifier from huggingface.co/models\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\"\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--tokenizer_name\",\n",
        "            default=None,\n",
        "            type=str,\n",
        "            help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--cache_dir\",\n",
        "            default=\"\",\n",
        "            type=str,\n",
        "            help=\"Where do you want to store the pre-trained models downloaded from s3\",\n",
        "        )\n",
        "        parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
        "        parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
        "        parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
        "        parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
        "        parser.add_argument(\"--num_workers\", default=4, type=int, help=\"kwarg passed to DataLoader\")\n",
        "        parser.add_argument(\"--num_train_epochs\", dest=\"max_epochs\", default=3, type=int)\n",
        "        parser.add_argument(\"--train_batch_size\", default=32, type=int)\n",
        "        parser.add_argument(\"--eval_batch_size\", default=32, type=int)\n",
        "\n",
        "\n",
        "class LoggingCallback(pl.Callback):\n",
        "    def on_batch_end(self, trainer, pl_module):\n",
        "        lrs = {f\"lr_group_{i}\": lr for i, lr in enumerate(self.lr_scheduler.get_lr())}\n",
        "        pl_module.logger.log_metrics(lrs)\n",
        "\n",
        "    def on_validation_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        rank_zero_info(\"***** Validation results *****\")\n",
        "        metrics = trainer.callback_metrics\n",
        "        # Log results\n",
        "        for key in sorted(metrics):\n",
        "            if key not in [\"log\", \"progress_bar\"]:\n",
        "                rank_zero_info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "\n",
        "    def on_test_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        rank_zero_info(\"***** Test results *****\")\n",
        "        metrics = trainer.callback_metrics\n",
        "        # Log and save results to file\n",
        "        output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n",
        "        with open(output_test_results_file, \"w\") as writer:\n",
        "            for key in sorted(metrics):\n",
        "                if key not in [\"log\", \"progress_bar\"]:\n",
        "                    rank_zero_info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "                    writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "\n",
        "\n",
        "def add_generic_args(parser, root_dir) -> None:\n",
        "    #  TODO(SS): allow all pl args? parser = pl.Trainer.add_argparse_args(parser)\n",
        "    parser.add_argument(\n",
        "        \"--output_dir\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--fp16\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--fp16_opt_level\",\n",
        "        type=str,\n",
        "        default=\"O2\",\n",
        "        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "        \"See details at https://nvidia.github.io/apex/amp.html\",\n",
        "    )\n",
        "    parser.add_argument(\"--n_tpu_cores\", dest=\"tpu_cores\", type=int, default=0)\n",
        "    parser.add_argument(\"--max_grad_norm\", dest=\"gradient_clip_val\", default=1.0, type=float, help=\"Max gradient norm\")\n",
        "    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
        "    parser.add_argument(\"--do_predict\", action=\"store_true\", help=\"Whether to run predictions on the test set.\")\n",
        "    parser.add_argument(\n",
        "        \"--gradient_accumulation_steps\",\n",
        "        dest=\"accumulate_grad_batches\",\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n",
        "\n",
        "\n",
        "def generic_train(\n",
        "    model: BaseTransformer,\n",
        "    args: argparse.Namespace,\n",
        "    early_stopping_callback=False,\n",
        "    logger=True,  # can pass WandbLogger() here\n",
        "    extra_callbacks=[],\n",
        "    checkpoint_callback=None,\n",
        "    logging_callback=None,\n",
        "    **extra_train_kwargs\n",
        "):\n",
        "    pl.seed_everything(args.seed)\n",
        "\n",
        "    # init model\n",
        "    odir = Path(model.hparams.output_dir)\n",
        "    odir.mkdir(exist_ok=True)\n",
        "\n",
        "    # add custom checkpoints\n",
        "    if checkpoint_callback is None:\n",
        "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "            filepath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=1\n",
        "        )\n",
        "    if logging_callback is None:\n",
        "        logging_callback = LoggingCallback()\n",
        "\n",
        "    train_params = {}\n",
        "\n",
        "    # TODO: remove with PyTorch 1.6 since pl uses native amp\n",
        "    if args.fp16:\n",
        "        train_params[\"precision\"] = 16\n",
        "        train_params[\"amp_level\"] = args.fp16_opt_level\n",
        "\n",
        "    if args.gpus > 1:\n",
        "        train_params[\"distributed_backend\"] = \"ddp\"\n",
        "\n",
        "    trainer = pl.Trainer.from_argparse_args(\n",
        "        args,\n",
        "        weights_summary=None,\n",
        "        callbacks=[logging_callback] + extra_callbacks,\n",
        "        logger=logger,\n",
        "        checkpoint_callback=checkpoint_callback,\n",
        "        early_stop_callback=early_stopping_callback,\n",
        "        **train_params,\n",
        "    )\n",
        "\n",
        "    if args.do_train:\n",
        "        trainer.fit(model)\n",
        "\n",
        "    return trainer"
      ],
      "metadata": {
        "id": "m1bnaw72DYFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Callbacks"
      ],
      "metadata": {
        "id": "jkzV9CVRES7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.utilities import rank_zero_only\n",
        "\n",
        "\n",
        "def count_trainable_parameters(model):\n",
        "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "    return params\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class Seq2SeqLoggingCallback(pl.Callback):\n",
        "    @rank_zero_only\n",
        "    def _write_logs(\n",
        "        self, trainer: pl.Trainer, pl_module: pl.LightningModule, type_path: str, save_generations=True\n",
        "    ) -> None:\n",
        "        logger.info(f\"***** {type_path} results at step {trainer.global_step:05d} *****\")\n",
        "        metrics = trainer.callback_metrics\n",
        "        trainer.logger.log_metrics({k: v for k, v in metrics.items() if k not in [\"log\", \"progress_bar\", \"preds\"]})\n",
        "        # Log results\n",
        "        od = Path(pl_module.hparams.output_dir)\n",
        "        if type_path == \"test\":\n",
        "            results_file = od / \"test_results.txt\"\n",
        "            generations_file = od / \"test_generations.txt\"\n",
        "        else:\n",
        "            # this never gets hit. I prefer not to save intermediate generations, and results are in metrics.json\n",
        "            # If people want this it will be easy enough to add back.\n",
        "            results_file = od / f\"{type_path}_results/{trainer.global_step:05d}.txt\"\n",
        "            generations_file = od / f\"{type_path}_generations/{trainer.global_step:05d}.txt\"\n",
        "            results_file.parent.mkdir(exist_ok=True)\n",
        "            generations_file.parent.mkdir(exist_ok=True)\n",
        "        with open(results_file, \"a+\") as writer:\n",
        "            for key in sorted(metrics):\n",
        "                if key in [\"log\", \"progress_bar\", \"preds\"]:\n",
        "                    continue\n",
        "                val = metrics[key]\n",
        "                if isinstance(val, torch.Tensor):\n",
        "                    val = val.item()\n",
        "                msg = f\"{key}: {val:.6f}\\n\"\n",
        "                writer.write(msg)\n",
        "\n",
        "        if not save_generations:\n",
        "            return\n",
        "\n",
        "        if \"preds\" in metrics:\n",
        "            content = \"\\n\".join(metrics[\"preds\"])\n",
        "            generations_file.open(\"w+\").write(content)\n",
        "\n",
        "    @rank_zero_only\n",
        "    def on_train_start(self, trainer, pl_module):\n",
        "        try:\n",
        "            npars = pl_module.model.model.num_parameters()\n",
        "        except AttributeError:\n",
        "            npars = pl_module.model.num_parameters()\n",
        "\n",
        "        n_trainable_pars = count_trainable_parameters(pl_module)\n",
        "        # mp stands for million parameters\n",
        "        trainer.logger.log_metrics({\"n_params\": npars, \"mp\": npars / 1e6, \"grad_mp\": n_trainable_pars / 1e6})\n",
        "\n",
        "    @rank_zero_only\n",
        "    def on_test_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        return self._write_logs(trainer, pl_module, \"test\")\n",
        "\n",
        "\n",
        "def get_checkpoint_callback(output_dir, metric):\n",
        "    \"\"\"Saves the best model by validation ROUGE2 score.\"\"\"\n",
        "    if metric == \"rouge2\":\n",
        "        exp = \"{val_avg_rouge2:.4f}-{step_count}\"\n",
        "    elif metric == \"bleu\":\n",
        "        exp = \"{val_avg_bleu:.4f}-{step_count}\"\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            f\"seq2seq callbacks only support rouge2 and bleu, got {metric}, You can make your own by adding to this function.\"\n",
        "        )\n",
        "\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        filepath=os.path.join(output_dir, exp),\n",
        "        monitor=f\"val_{metric}\",\n",
        "        mode=\"max\",\n",
        "        save_top_k=1,\n",
        "        period=0,  # maybe save a checkpoint every time val is run, not just end of epoch.\n",
        "    )\n",
        "    return checkpoint_callback"
      ],
      "metadata": {
        "id": "v011XH25EWff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "vJS2-ujDF9nV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import json\n",
        "import linecache\n",
        "import os\n",
        "import pickle\n",
        "import warnings\n",
        "from logging import getLogger\n",
        "from pathlib import Path\n",
        "from typing import Callable, Dict, Iterable, List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from rouge_score import rouge_scorer, scoring\n",
        "from sacrebleu import corpus_bleu\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, Sampler\n",
        "\n",
        "from transformers import BartTokenizer\n",
        "\n",
        "\n",
        "def encode_line(tokenizer, line, max_length, pad_to_max_length=True, return_tensors=\"pt\"):\n",
        "    extra_kw = {\"add_prefix_space\": True} if isinstance(tokenizer, BartTokenizer) else {}\n",
        "    return tokenizer(\n",
        "        [line],\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\" if pad_to_max_length else None,\n",
        "        truncation=True,\n",
        "        return_tensors=return_tensors,\n",
        "        **extra_kw,\n",
        "    )\n",
        "\n",
        "\n",
        "def lmap(f: Callable, x: Iterable) -> List:\n",
        "    \"\"\"list(map(f, x))\"\"\"\n",
        "    return list(map(f, x))\n",
        "\n",
        "\n",
        "def calculate_bleu_score(output_lns, refs_lns, **kwargs) -> dict:\n",
        "    \"\"\"Uses sacrebleu's corpus_bleu implementation.\"\"\"\n",
        "    return {\"bleu\": corpus_bleu(output_lns, [refs_lns], **kwargs).score}\n",
        "\n",
        "\n",
        "def trim_batch(\n",
        "    input_ids, pad_token_id, attention_mask=None,\n",
        "):\n",
        "    \"\"\"Remove columns that are populated exclusively by pad_token_id\"\"\"\n",
        "    keep_column_mask = input_ids.ne(pad_token_id).any(dim=0)\n",
        "    if attention_mask is None:\n",
        "        return input_ids[:, keep_column_mask]\n",
        "    else:\n",
        "        return (input_ids[:, keep_column_mask], attention_mask[:, keep_column_mask])\n",
        "\n",
        "\n",
        "class Seq2SeqDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer,\n",
        "        data_dir,\n",
        "        max_source_length,\n",
        "        max_target_length,\n",
        "        type_path=\"train\",\n",
        "        n_obs=None,\n",
        "        src_lang=None,\n",
        "        tgt_lang=None,\n",
        "        prefix=\"\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.src_file = Path(data_dir).joinpath(type_path + \".source\")\n",
        "        self.tgt_file = Path(data_dir).joinpath(type_path + \".target\")\n",
        "        self.src_lens = self.get_char_lens(self.src_file)\n",
        "        self.max_source_length = max_source_length\n",
        "        self.max_target_length = max_target_length\n",
        "        assert min(self.src_lens) > 0, f\"found empty line in {self.src_file}\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.prefix = prefix\n",
        "        if n_obs is not None:\n",
        "            self.src_lens = self.src_lens[:n_obs]\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_lens)\n",
        "\n",
        "    def __getitem__(self, index) -> Dict[str, torch.Tensor]:\n",
        "        index = index + 1  # linecache starts at 1\n",
        "        source_line = self.prefix + linecache.getline(str(self.src_file), index).rstrip(\"\\n\")\n",
        "        tgt_line = linecache.getline(str(self.tgt_file), index).rstrip(\"\\n\")\n",
        "        assert source_line, f\"empty source line for index {index}\"\n",
        "        assert tgt_line, f\"empty tgt line for index {index}\"\n",
        "        source_inputs = encode_line(self.tokenizer, source_line, self.max_source_length)\n",
        "        target_inputs = encode_line(self.tokenizer, tgt_line, self.max_target_length)\n",
        "\n",
        "        source_ids = source_inputs[\"input_ids\"].squeeze()\n",
        "        target_ids = target_inputs[\"input_ids\"].squeeze()\n",
        "        src_mask = source_inputs[\"attention_mask\"].squeeze()\n",
        "        return {\n",
        "            \"input_ids\": source_ids,\n",
        "            \"attention_mask\": src_mask,\n",
        "            \"decoder_input_ids\": target_ids,\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def get_char_lens(data_file):\n",
        "        return [len(x) for x in Path(data_file).open().readlines()]\n",
        "\n",
        "    @staticmethod\n",
        "    def trim_seq2seq_batch(batch, pad_token_id) -> tuple:\n",
        "        y = trim_batch(batch[\"decoder_input_ids\"], pad_token_id)\n",
        "        source_ids, source_mask = trim_batch(batch[\"input_ids\"], pad_token_id, attention_mask=batch[\"attention_mask\"])\n",
        "        return source_ids, source_mask, y\n",
        "\n",
        "    def collate_fn(self, batch) -> Dict[str, torch.Tensor]:\n",
        "        input_ids = torch.stack([x[\"input_ids\"] for x in batch])\n",
        "        masks = torch.stack([x[\"attention_mask\"] for x in batch])\n",
        "        target_ids = torch.stack([x[\"decoder_input_ids\"] for x in batch])\n",
        "        pad_token_id = self.pad_token_id\n",
        "        y = trim_batch(target_ids, pad_token_id)\n",
        "        source_ids, source_mask = trim_batch(input_ids, pad_token_id, attention_mask=masks)\n",
        "        batch = {\n",
        "            \"input_ids\": source_ids,\n",
        "            \"attention_mask\": source_mask,\n",
        "            \"decoder_input_ids\": y,\n",
        "        }\n",
        "        return batch\n",
        "\n",
        "    def make_sortish_sampler(self, batch_size):\n",
        "        return SortishSampler(self.src_lens, batch_size)\n",
        "\n",
        "\n",
        "class MBartDataset(Seq2SeqDataset):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        if self.max_source_length != self.max_target_length:\n",
        "            warnings.warn(\n",
        "                f\"Mbart will ignore max_target_length = {self.max_target_length} and use {self.max_source_length} for both sides.\"\n",
        "            )\n",
        "\n",
        "    def __getitem__(self, index) -> Dict[str, str]:\n",
        "        index = index + 1  # linecache starts at 1\n",
        "        source_line = self.prefix + linecache.getline(str(self.src_file), index).rstrip(\"\\n\")\n",
        "        tgt_line = linecache.getline(str(self.tgt_file), index).rstrip(\"\\n\")\n",
        "        assert source_line, f\"empty source line for index {index}\"\n",
        "        assert tgt_line, f\"empty tgt line for index {index}\"\n",
        "        return {\n",
        "            \"tgt_texts\": source_line,\n",
        "            \"src_texts\": tgt_line,\n",
        "        }\n",
        "\n",
        "    def collate_fn(self, batch) -> Dict[str, torch.Tensor]:\n",
        "        batch_encoding = self.tokenizer.prepare_translation_batch(\n",
        "            [x[\"src_texts\"] for x in batch],\n",
        "            src_lang=self.src_lang,\n",
        "            tgt_texts=[x[\"tgt_texts\"] for x in batch],\n",
        "            tgt_lang=self.tgt_lang,\n",
        "            max_length=self.max_source_length,\n",
        "        )\n",
        "        return batch_encoding.data\n",
        "\n",
        "\n",
        "class SortishSampler(Sampler):\n",
        "    \"Go through the text data by order of src length with a bit of randomness. From fastai repo.\"\n",
        "\n",
        "    def __init__(self, data, batch_size):\n",
        "        self.data, self.bs = data, batch_size\n",
        "\n",
        "    def key(self, i):\n",
        "        return self.data[i]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __iter__(self):\n",
        "        idxs = np.random.permutation(len(self.data))\n",
        "        sz = self.bs * 50\n",
        "        ck_idx = [idxs[i : i + sz] for i in range(0, len(idxs), sz)]\n",
        "        sort_idx = np.concatenate([sorted(s, key=self.key, reverse=True) for s in ck_idx])\n",
        "        sz = self.bs\n",
        "        ck_idx = [sort_idx[i : i + sz] for i in range(0, len(sort_idx), sz)]\n",
        "        max_ck = np.argmax([self.key(ck[0]) for ck in ck_idx])  # find the chunk with the largest key,\n",
        "        ck_idx[0], ck_idx[max_ck] = ck_idx[max_ck], ck_idx[0]  # then make sure it goes first.\n",
        "        sort_idx = np.concatenate(np.random.permutation(ck_idx[1:])) if len(ck_idx) > 1 else np.array([], dtype=np.int)\n",
        "        sort_idx = np.concatenate((ck_idx[0], sort_idx))\n",
        "        return iter(sort_idx)\n",
        "\n",
        "\n",
        "logger = getLogger(__name__)\n",
        "\n",
        "\n",
        "def use_task_specific_params(model, task):\n",
        "    \"\"\"Update config with summarization specific params.\"\"\"\n",
        "    task_specific_params = model.config.task_specific_params\n",
        "\n",
        "    if task_specific_params is not None:\n",
        "        pars = task_specific_params.get(task, {})\n",
        "        logger.info(f\"using task specific params for {task}: {pars}\")\n",
        "        model.config.update(pars)\n",
        "\n",
        "\n",
        "def pickle_load(path):\n",
        "    \"\"\"pickle.load(path)\"\"\"\n",
        "    with open(path, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "\n",
        "def pickle_save(obj, path):\n",
        "    \"\"\"pickle.dump(obj, path)\"\"\"\n",
        "    with open(path, \"wb\") as f:\n",
        "        return pickle.dump(obj, f)\n",
        "\n",
        "\n",
        "def flatten_list(summary_ids: List[List]):\n",
        "    return [x for x in itertools.chain.from_iterable(summary_ids)]\n",
        "\n",
        "\n",
        "def save_git_info(folder_path: str) -> None:\n",
        "    \"\"\"Save git information to output_dir/git_log.json\"\"\"\n",
        "    repo_infos = get_git_info()\n",
        "    save_json(repo_infos, os.path.join(folder_path, \"git_log.json\"))\n",
        "\n",
        "\n",
        "def save_json(content, path):\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(content, f, indent=4)\n",
        "\n",
        "\n",
        "def load_json(path):\n",
        "    with open(path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def get_git_info():\n",
        "    repo = git.Repo(search_parent_directories=True)\n",
        "    repo_infos = {\n",
        "        \"repo_id\": str(repo),\n",
        "        \"repo_sha\": str(repo.head.object.hexsha),\n",
        "        \"repo_branch\": str(repo.active_branch),\n",
        "    }\n",
        "    return repo_infos\n",
        "\n",
        "\n",
        "ROUGE_KEYS = [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
        "\n",
        "\n",
        "def calculate_rouge(output_lns: List[str], reference_lns: List[str], use_stemmer=True) -> Dict:\n",
        "    scorer = rouge_scorer.RougeScorer(ROUGE_KEYS, use_stemmer=use_stemmer)\n",
        "    aggregator = scoring.BootstrapAggregator()\n",
        "\n",
        "    for reference_ln, output_ln in zip(reference_lns, output_lns):\n",
        "        scores = scorer.score(reference_ln, output_ln)\n",
        "        aggregator.add_scores(scores)\n",
        "\n",
        "    result = aggregator.aggregate()\n",
        "    return {k: v.mid.fmeasure for k, v in result.items()}\n",
        "\n",
        "\n",
        "def freeze_params(model: nn.Module):\n",
        "    for par in model.parameters():\n",
        "        par.requires_grad = False\n",
        "\n",
        "\n",
        "def grad_status(model: nn.Module) -> Iterable:\n",
        "    return (par.requires_grad for par in model.parameters())\n",
        "\n",
        "\n",
        "def any_requires_grad(model: nn.Module) -> bool:\n",
        "    return any(grad_status(model))\n",
        "\n",
        "\n",
        "def assert_all_frozen(model):\n",
        "    model_grads: List[bool] = list(grad_status(model))\n",
        "    n_require_grad = sum(lmap(int, model_grads))\n",
        "    npars = len(model_grads)\n",
        "    assert not any(model_grads), f\"{n_require_grad/npars:.1%} of {npars} weights require grad\"\n",
        "\n",
        "\n",
        "def assert_not_all_frozen(model):\n",
        "    model_grads: List[bool] = list(grad_status(model))\n",
        "    npars = len(model_grads)\n",
        "    assert any(model_grads), f\"none of {npars} weights require grad\""
      ],
      "metadata": {
        "id": "N5gD_SPvSzlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tune"
      ],
      "metadata": {
        "id": "AuJvPdg2EBuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# from lightning_base import BaseTransformer, add_generic_args, generic_train\n",
        "# from transformers import MBartTokenizer, get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "# try:\n",
        "#     from .utils import (\n",
        "#         assert_all_frozen,\n",
        "#         use_task_specific_params,\n",
        "#         lmap,\n",
        "#         flatten_list,\n",
        "#         pickle_save,\n",
        "#         save_git_info,\n",
        "#         save_json,\n",
        "#         freeze_params,\n",
        "#         calculate_rouge,\n",
        "#         get_git_info,\n",
        "#         ROUGE_KEYS,\n",
        "#         calculate_bleu_score,\n",
        "#         Seq2SeqDataset,\n",
        "#         MBartDataset,\n",
        "#     )\n",
        "\n",
        "#     from .callbacks import Seq2SeqLoggingCallback, get_checkpoint_callback\n",
        "# except ImportError:\n",
        "#     from utils import (\n",
        "#         Seq2SeqDataset,\n",
        "#         MBartDataset,\n",
        "#         assert_all_frozen,\n",
        "#         use_task_specific_params,\n",
        "#         lmap,\n",
        "#         flatten_list,\n",
        "#         pickle_save,\n",
        "#         save_git_info,\n",
        "#         save_json,\n",
        "#         freeze_params,\n",
        "#         calculate_rouge,\n",
        "#         get_git_info,\n",
        "#         ROUGE_KEYS,\n",
        "#         calculate_bleu_score,\n",
        "#     )\n",
        "#     from callbacks import Seq2SeqLoggingCallback, get_checkpoint_callback\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class SummarizationModule(BaseTransformer):\n",
        "    mode = \"summarization\"\n",
        "    loss_names = [\"loss\"]\n",
        "    metric_names = ROUGE_KEYS\n",
        "    val_metric = \"rouge2\"\n",
        "\n",
        "    def __init__(self, hparams, **kwargs):\n",
        "        super().__init__(hparams, num_labels=None, mode=self.mode, **kwargs)\n",
        "        #use_task_specific_params(self.model, \"summarization\")\n",
        "        #save_git_info(self.hparams.output_dir)\n",
        "        #self.metrics_save_path = Path(\"/results/metrics.json\")\n",
        "        self.metrics_save_path = Path(self.output_dir) / \"metrics.json\"\n",
        "        self.hparams_save_path = Path(self.output_dir) / \"hparams.pkl\"\n",
        "        pickle_save(self.hparams, self.hparams_save_path)\n",
        "        self.step_count = 0\n",
        "        self.metrics = defaultdict(list)\n",
        "\n",
        "        self.dataset_kwargs: dict = dict(\n",
        "            data_dir=self.hparams.data_dir,\n",
        "            max_source_length=self.hparams.max_source_length,\n",
        "            prefix=self.model.config.prefix or \"\",\n",
        "        )\n",
        "        n_observations_per_split = {\n",
        "            \"train\": self.hparams.n_train,\n",
        "            \"val\": self.hparams.n_val,\n",
        "            \"test\": self.hparams.n_test,\n",
        "        }\n",
        "        self.n_obs = {k: v if v >= 0 else None for k, v in n_observations_per_split.items()}\n",
        "\n",
        "        self.target_lens = {\n",
        "            \"train\": self.hparams.max_target_length,\n",
        "            \"val\": self.hparams.val_max_target_length,\n",
        "            \"test\": self.hparams.test_max_target_length,\n",
        "        }\n",
        "        assert self.target_lens[\"train\"] <= self.target_lens[\"val\"], f\"target_lens: {self.target_lens}\"\n",
        "        assert self.target_lens[\"train\"] <= self.target_lens[\"test\"], f\"target_lens: {self.target_lens}\"\n",
        "\n",
        "        if self.hparams.freeze_embeds:\n",
        "            self.freeze_embeds()\n",
        "        if self.hparams.freeze_encoder:\n",
        "            freeze_params(self.model.get_encoder())\n",
        "            assert_all_frozen(self.model.get_encoder())\n",
        "\n",
        "        #self.hparams.git_sha = get_git_info()[\"repo_sha\"]\n",
        "        try:\n",
        "            self.num_workers = hparams.num_workers\n",
        "        except AttributeError:\n",
        "            self.num_workers = 2\n",
        "\n",
        "        self.decoder_start_token_id = None\n",
        "        self.dataset_class = Seq2SeqDataset\n",
        "\n",
        "    def freeze_embeds(self):\n",
        "        \"\"\"Freeze token embeddings and positional embeddings for bart, just token embeddings for t5.\"\"\"\n",
        "        try:\n",
        "            freeze_params(self.model.model.shared)\n",
        "            for d in [self.model.model.encoder, self.model.model.decoder]:\n",
        "                freeze_params(d.embed_positions)\n",
        "                freeze_params(d.embed_tokens)\n",
        "        except AttributeError:\n",
        "            freeze_params(self.model.shared)\n",
        "            for d in [self.model.encoder, self.model.decoder]:\n",
        "                freeze_params(d.embed_tokens)\n",
        "\n",
        "    def forward(self, input_ids, **kwargs):\n",
        "        return self.model(input_ids, **kwargs)\n",
        "\n",
        "    def ids_to_clean_text(self, generated_ids: List[int]):\n",
        "        gen_text = self.tokenizer.batch_decode(\n",
        "            generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "        )\n",
        "        return lmap(str.strip, gen_text)\n",
        "\n",
        "    def _step(self, batch: dict) -> Tuple:\n",
        "        pad_token_id = self.tokenizer.pad_token_id\n",
        "        source_ids, source_mask, y = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"decoder_input_ids\"]\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone()\n",
        "        lm_labels[y[:, 1:] == pad_token_id] = -100\n",
        "        outputs = self(source_ids, attention_mask=source_mask, decoder_input_ids=y_ids, labels=lm_labels,)\n",
        "        loss = outputs[0]\n",
        "        return (loss,)\n",
        "\n",
        "    def training_step(self, batch, batch_idx) -> Dict:\n",
        "        loss_tensors = self._step(batch)\n",
        "        logs = {name: loss for name, loss in zip(self.loss_names, loss_tensors)}\n",
        "        return {\"loss\": loss_tensors[0], \"log\": logs}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx) -> Dict:\n",
        "        return self._generative_step(batch)\n",
        "\n",
        "    def validation_epoch_end(self, outputs, prefix=\"val\") -> Dict:\n",
        "        self.step_count += 1\n",
        "        losses = {k: torch.stack([x[k] for x in outputs]).mean() for k in self.loss_names}\n",
        "        loss = losses[\"loss\"]\n",
        "        rouges = {k: np.array([x[k] for x in outputs]).mean() for k in self.metric_names + [\"gen_time\", \"summ_len\"]}\n",
        "        rouge_tensor: torch.FloatTensor = torch.tensor(rouges[self.val_metric]).type_as(loss)\n",
        "        rouges.update({k: v.item() for k, v in losses.items()})\n",
        "        losses.update(rouges)\n",
        "        metrics = {f\"{prefix}_avg_{k}\": x for k, x in losses.items()}\n",
        "        metrics[\"avg_rouge1\"] = losses['rouge1']\n",
        "        metrics[\"step_count\"] = self.step_count\n",
        "        self.save_metrics(metrics, prefix)  # writes to self.metrics_save_path\n",
        "        preds = flatten_list([x[\"preds\"] for x in outputs])\n",
        "        return {\"log\": metrics, \"preds\": preds, f\"{prefix}_loss\": loss, f\"{prefix}_{self.val_metric}\": rouge_tensor}\n",
        "\n",
        "    def save_metrics(self, latest_metrics, type_path) -> None:\n",
        "        self.metrics[type_path].append(latest_metrics)\n",
        "        save_json(self.metrics, self.metrics_save_path)\n",
        "\n",
        "    def calc_generative_metrics(self, preds, target) -> Dict:\n",
        "        return calculate_rouge(preds, target)\n",
        "\n",
        "    def _generative_step(self, batch: dict) -> dict:\n",
        "        pad_token_id = self.tokenizer.pad_token_id\n",
        "        source_ids, source_mask, y = Seq2SeqDataset.trim_seq2seq_batch(batch, pad_token_id)\n",
        "        t0 = time.time()\n",
        "        generated_ids = self.model.generate(\n",
        "            input_ids=source_ids,\n",
        "            attention_mask=source_mask,\n",
        "            use_cache=True,\n",
        "            decoder_start_token_id=self.decoder_start_token_id,\n",
        "        )\n",
        "        gen_time = (time.time() - t0) / source_ids.shape[0]\n",
        "        preds = self.ids_to_clean_text(generated_ids)\n",
        "        target = self.ids_to_clean_text(y)\n",
        "        loss_tensors = self._step(batch)\n",
        "        base_metrics = {name: loss for name, loss in zip(self.loss_names, loss_tensors)}\n",
        "        rouge: Dict = self.calc_generative_metrics(preds, target)\n",
        "        summ_len = np.mean(lmap(len, generated_ids))\n",
        "        base_metrics.update(gen_time=gen_time, summ_len=summ_len, preds=preds, target=target, **rouge)\n",
        "        return base_metrics\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        return self._generative_step(batch)\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        return self.validation_epoch_end(outputs, prefix=\"test\")\n",
        "\n",
        "    def get_dataset(self, type_path) -> Seq2SeqDataset:\n",
        "        n_obs = self.n_obs[type_path]\n",
        "        max_target_length = self.target_lens[type_path]\n",
        "        dataset = self.dataset_class(\n",
        "            self.tokenizer,\n",
        "            type_path=type_path,\n",
        "            n_obs=n_obs,\n",
        "            max_target_length=max_target_length,\n",
        "            **self.dataset_kwargs,\n",
        "        )\n",
        "        return dataset\n",
        "\n",
        "    def get_dataloader(self, type_path: str, batch_size: int, shuffle: bool = False) -> DataLoader:\n",
        "        dataset = self.get_dataset(type_path)\n",
        "        sampler = None\n",
        "        if self.hparams.sortish_sampler and type_path == \"train\":\n",
        "            assert self.hparams.gpus <= 1\n",
        "            sampler = dataset.make_sortish_sampler(batch_size)\n",
        "            shuffle = False\n",
        "\n",
        "        dataloader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=batch_size,\n",
        "            collate_fn=dataset.collate_fn,\n",
        "            shuffle=shuffle,\n",
        "            num_workers=self.num_workers,\n",
        "            sampler=sampler,\n",
        "        )\n",
        "        return dataloader\n",
        "\n",
        "    def train_dataloader(self) -> DataLoader:\n",
        "        dataloader = self.get_dataloader(\"train\", batch_size=self.hparams.train_batch_size, shuffle=True)\n",
        "        t_total = (\n",
        "            (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.gpus)))\n",
        "            // self.hparams.accumulate_grad_batches\n",
        "            * float(self.hparams.max_epochs)\n",
        "        )\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
        "        )\n",
        "        if max(scheduler.get_last_lr()) > 0:\n",
        "            warnings.warn(\"All learning rates are 0\")\n",
        "        self.lr_scheduler = scheduler\n",
        "        return dataloader\n",
        "\n",
        "    def val_dataloader(self) -> DataLoader:\n",
        "        return self.get_dataloader(\"val\", batch_size=self.hparams.eval_batch_size)\n",
        "\n",
        "    def test_dataloader(self) -> DataLoader:\n",
        "        return self.get_dataloader(\"test\", batch_size=self.hparams.eval_batch_size)\n",
        "\n",
        "    @staticmethod\n",
        "    def add_model_specific_args(parser, root_dir):\n",
        "        BaseTransformer.add_model_specific_args(parser, root_dir)\n",
        "        add_generic_args(parser, root_dir)\n",
        "        parser.add_argument(\n",
        "            \"--max_source_length\",\n",
        "            default=48,\n",
        "            type=int,\n",
        "            help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "            \"than this will be truncated, sequences shorter will be padded.\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--max_target_length\",\n",
        "            default=24,\n",
        "            type=int,\n",
        "            help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "            \"than this will be truncated, sequences shorter will be padded.\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--val_max_target_length\",\n",
        "            default=24,\n",
        "            type=int,\n",
        "            help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "            \"than this will be truncated, sequences shorter will be padded.\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--test_max_target_length\",\n",
        "            default=24,\n",
        "            type=int,\n",
        "            help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "            \"than this will be truncated, sequences shorter will be padded.\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--data_dir\",\n",
        "            type=str,\n",
        "            required=True,\n",
        "            help=\"The input data dir. Should contain train.source, train.target, val.source, val.target, test.source, test.target\",\n",
        "        )\n",
        "        parser.add_argument(\"--freeze_encoder\", action=\"store_true\")\n",
        "        parser.add_argument(\"--freeze_embeds\", action=\"store_true\")\n",
        "        parser.add_argument(\"--sortish_sampler\", action=\"store_true\", default=False)\n",
        "        parser.add_argument(\"--logger_name\", type=str, choices=[\"default\", \"wandb\", \"wandb_shared\"], default=\"default\")\n",
        "        parser.add_argument(\"--n_train\", type=int, default=-1, required=False, help=\"# examples. -1 means use all.\")\n",
        "        parser.add_argument(\"--n_val\", type=int, default=500, required=False, help=\"# examples. -1 means use all.\")\n",
        "        parser.add_argument(\"--n_test\", type=int, default=-1, required=False, help=\"# examples. -1 means use all.\")\n",
        "        parser.add_argument(\n",
        "            \"--task\", type=str, default=\"summarization\", required=False, help=\"# examples. -1 means use all.\"\n",
        "        )\n",
        "        parser.add_argument(\"--src_lang\", type=str, default=\"\", required=False)\n",
        "        parser.add_argument(\"--tgt_lang\", type=str, default=\"\", required=False)\n",
        "        parser.add_argument(\"--atomic\", action=\"store_true\")\n",
        "        return parser\n",
        "\n",
        "\n",
        "class TranslationModule(SummarizationModule):\n",
        "    mode = \"translation\"\n",
        "    loss_names = [\"loss\"]\n",
        "    metric_names = [\"bleu\"]\n",
        "    val_metric = \"bleu\"\n",
        "\n",
        "    def __init__(self, hparams, **kwargs):\n",
        "        super().__init__(hparams, **kwargs)\n",
        "        self.dataset_kwargs[\"src_lang\"] = hparams.src_lang\n",
        "        self.dataset_kwargs[\"tgt_lang\"] = hparams.tgt_lang\n",
        "        if self.model.config.decoder_start_token_id is None and isinstance(self.tokenizer, MBartTokenizer):\n",
        "            self.decoder_start_token_id = self.tokenizer.lang_code_to_id[hparams.tgt_lang]\n",
        "        if isinstance(self.tokenizer, MBartTokenizer):\n",
        "            self.dataset_class = MBartDataset\n",
        "\n",
        "    def calc_generative_metrics(self, preds, target) -> dict:\n",
        "        return calculate_bleu_score(preds, target)\n",
        "\n",
        "\n",
        "def main(args, model=None) -> SummarizationModule:\n",
        "    Path(args.output_dir).mkdir(exist_ok=True)\n",
        "    if len(os.listdir(args.output_dir)) > 3 and args.do_train:\n",
        "        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n",
        "    if model is None:\n",
        "        if args.task == \"summarization\":\n",
        "            model: SummarizationModule = SummarizationModule(args)\n",
        "        else:\n",
        "            model: SummarizationModule = TranslationModule(args)\n",
        "\n",
        "    ### add atomic relation tokens\n",
        "    if args.atomic:\n",
        "        print(\"Special tokens are added.\")\n",
        "\n",
        "        additional_tokens_list = [\n",
        "            \"AtLocation\",\n",
        "            \"CapableOf\",\n",
        "            \"Causes\",\n",
        "            \"CausesDesire\",\n",
        "            \"CreatedBy\",\n",
        "            \"DefinedAs\",\n",
        "            \"DesireOf\",\n",
        "            \"Desires\",\n",
        "            \"HasA\",\n",
        "            \"HasFirstSubevent\",\n",
        "            \"HasLastSubevent\",\n",
        "            \"HasPainCharacter\",\n",
        "            \"HasPainIntensity\",\n",
        "            \"HasPrerequisite\",\n",
        "            \"HasProperty\",\n",
        "            \"HasSubEvent\",\n",
        "            \"HasSubevent\",\n",
        "            \"HinderedBy\",\n",
        "            \"InheritsFrom\",\n",
        "            \"InstanceOf\",\n",
        "            \"IsA\",\n",
        "            \"LocatedNear\",\n",
        "            \"LocationOfAction\",\n",
        "            \"MadeOf\",\n",
        "            \"MadeUpOf\",\n",
        "            \"MotivatedByGoal\",\n",
        "            \"NotCapableOf\",\n",
        "            \"NotDesires\",\n",
        "            \"NotHasA\",\n",
        "            \"NotHasProperty\",\n",
        "            \"NotIsA\",\n",
        "            \"NotMadeOf\",\n",
        "            \"ObjectUse\",\n",
        "            \"PartOf\",\n",
        "            \"ReceivesAction\",\n",
        "            \"RelatedTo\",\n",
        "            \"SymbolOf\",\n",
        "            \"UsedFor\",\n",
        "            \"isAfter\",\n",
        "            \"isBefore\",\n",
        "            \"isFilledBy\",\n",
        "            \"oEffect\",\n",
        "            \"oReact\",\n",
        "            \"oWant\",\n",
        "            \"xAttr\",\n",
        "            \"xEffect\",\n",
        "            \"xIntent\",\n",
        "            \"xNeed\",\n",
        "            \"xReact\",\n",
        "            \"xReason\",\n",
        "            \"xWant\",\n",
        "            ]\n",
        "\n",
        "        num_added_toks = model.tokenizer.add_tokens(additional_tokens_list)\n",
        "        model.model.resize_token_embeddings(len(model.tokenizer))\n",
        "\n",
        "    dataset = Path(args.data_dir).name\n",
        "    if (\n",
        "        args.logger_name == \"default\"\n",
        "        or args.fast_dev_run\n",
        "        or str(args.output_dir).startswith(\"/tmp\")\n",
        "        or str(args.output_dir).startswith(\"/var\")\n",
        "    ):\n",
        "        logger = True  # don't pollute wandb logs unnecessarily\n",
        "    elif args.logger_name == \"wandb\":\n",
        "        from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "        logger = WandbLogger(name=model.output_dir.name, project=dataset)\n",
        "\n",
        "    elif args.logger_name == \"wandb_shared\":\n",
        "        from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "        logger = WandbLogger(name=model.output_dir.name, project=f\"hf_{dataset}\")\n",
        "    trainer: pl.Trainer = generic_train(\n",
        "        model,\n",
        "        args,\n",
        "        logging_callback=Seq2SeqLoggingCallback(),\n",
        "        checkpoint_callback=get_checkpoint_callback(args.output_dir, model.val_metric),\n",
        "        logger=logger,\n",
        "    )\n",
        "    pickle_save(model.hparams, model.output_dir / \"hparams.pkl\")\n",
        "    if not args.do_predict:\n",
        "        return model\n",
        "\n",
        "    model.hparams.test_checkpoint = \"\"\n",
        "    checkpoints = list(sorted(glob.glob(os.path.join(args.output_dir, \"*.ckpt\"), recursive=True)))\n",
        "    if checkpoints:\n",
        "        model.hparams.test_checkpoint = checkpoints[-1]\n",
        "        trainer.resume_from_checkpoint = checkpoints[-1]\n",
        "    trainer.logger.log_hyperparams(model.hparams)\n",
        "\n",
        "    trainer.test(model)\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # parser = pl.Trainer.add_argparse_args(parser)\n",
        "    parser = SummarizationModule.add_model_specific_args(parser, os.getcwd())\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    trainer = pl.Trainer.from_argparse_args(args)\n",
        "\n",
        "    main(args)"
      ],
      "metadata": {
        "id": "HYhlXeQ_Dpcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distillation"
      ],
      "metadata": {
        "id": "YqV2bufsEINi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import gc\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class BartSummarizationDistiller(SummarizationModule):\n",
        "    loss_names = [\"loss\", \"ce_loss\", \"mlm_loss\", \"enc_mse_loss\", \"hid_loss_enc\", \"hid_loss_dec\"]\n",
        "\n",
        "    def __init__(self, hparams):\n",
        "        assert Path(hparams.data_dir).exists()\n",
        "        student, student_cfg, teacher = self.pre_init(hparams)\n",
        "\n",
        "        super().__init__(hparams, model=student, config=student_cfg)\n",
        "        self.teacher = teacher\n",
        "        use_task_specific_params(self.teacher, \"summarization\")\n",
        "        freeze_params(self.teacher)\n",
        "        self.sanity_check_gradients()\n",
        "        self.ce_loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "        self.temperature = 2.0\n",
        "        self.alpha_mlm = hparams.alpha_mlm\n",
        "        self.alpha_ce = hparams.alpha_ce\n",
        "        self.alpha_hid = hparams.alpha_hid\n",
        "        # self.alpha_cos = hparams.alpha_cos\n",
        "        self.alpha_encoder_loss = self.hparams.alpha_encoder_loss\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    def sanity_check_gradients(self):\n",
        "        assert_all_frozen(self.teacher)\n",
        "        assert_all_frozen(self.model.model.decoder.embed_tokens)\n",
        "        assert_all_frozen(self.model.model.encoder.embed_tokens)\n",
        "        if self.different_encoder:\n",
        "            assert any_requires_grad(self.model.model.encoder)\n",
        "        else:\n",
        "            freeze_params(self.model.model.encoder)\n",
        "            del self.teacher.model.encoder\n",
        "\n",
        "    def pre_init(self, hparams):\n",
        "        self.output_dir = Path(hparams.output_dir)\n",
        "        self.output_dir.mkdir(exist_ok=True)\n",
        "        teacher = BartForConditionalGeneration.from_pretrained(hparams.teacher).eval()\n",
        "        student_updates = {\n",
        "            \"decoder_layers\": hparams.student_decoder_layers,\n",
        "            \"encoder_layers\": hparams.student_encoder_layers,\n",
        "        }\n",
        "        if hparams.length_penalty != -1:\n",
        "            student_updates[\"length_penalty\"] = hparams.length_penalty\n",
        "        d_layers_to_copy = get_layers_to_copy(student_updates[\"decoder_layers\"], teacher.config.decoder_layers)\n",
        "        e_layers_to_copy: List = get_layers_to_copy(student_updates[\"encoder_layers\"], teacher.config.encoder_layers)\n",
        "        hparams.d_layer_to_copy = d_layers_to_copy\n",
        "        hparams.e_layer_to_copy = e_layers_to_copy\n",
        "        kw = teacher.config.to_diff_dict()\n",
        "        kw.update(student_updates)\n",
        "        # Copy weights\n",
        "        student_cfg = BartConfig(**kw)\n",
        "        student = BartForConditionalGeneration(student_cfg)\n",
        "        student, _ = init_student(student, teacher)\n",
        "        save_dir = self.output_dir.joinpath(\"student\")\n",
        "        self.copy_to_student(d_layers_to_copy, e_layers_to_copy, hparams, student, teacher)\n",
        "        student.save_pretrained(save_dir)\n",
        "        hparams.model_name_or_path = str(save_dir)\n",
        "        return student, student_cfg, teacher\n",
        "\n",
        "    def copy_to_student(self, d_layers_to_copy, e_layers_to_copy, hparams, student, teacher):\n",
        "        if teacher.config.model_type == \"t5\":\n",
        "            return self.copy_t5_to_student(d_layers_to_copy, e_layers_to_copy, hparams, student, teacher)\n",
        "        self.different_encoder: bool = hparams.student_encoder_layers != teacher.config.encoder_layers\n",
        "        self.different_decoder = hparams.student_decoder_layers != teacher.config.decoder_layers\n",
        "        if self.different_decoder:\n",
        "            copy_layers(teacher.model.decoder.layers, student.model.decoder.layers, d_layers_to_copy)\n",
        "        if self.different_encoder:\n",
        "            copy_layers(teacher.model.encoder.layers, student.model.encoder.layers, e_layers_to_copy)\n",
        "\n",
        "    def copy_t5_to_student(self, d_layers_to_copy, e_layers_to_copy, hparams, student, teacher):\n",
        "        self.different_encoder: bool = hparams.student_encoder_layers != teacher.config.num_layers\n",
        "        self.different_decoder = hparams.student_decoder_layers != teacher.config.num_layers\n",
        "        if self.different_decoder:\n",
        "            copy_layers(teacher.decoder.block, student.decoder.block, d_layers_to_copy)\n",
        "        if self.different_encoder:\n",
        "            copy_layers(teacher.encoder.block, student.encoder.block, e_layers_to_copy)\n",
        "\n",
        "    def calc_mse_loss(self, teacher_outputs: torch.Tensor, student_outputs: torch.Tensor, mask) -> torch.FloatTensor:\n",
        "        if mask is not None:\n",
        "            # mask has False at padding_idx\n",
        "            sel_mask = mask[:, :, None].expand_as(student_outputs).bool()\n",
        "            s_logits_slct = torch.masked_select(student_outputs, sel_mask)\n",
        "            t_logits_slct = torch.masked_select(teacher_outputs, sel_mask)\n",
        "        else:\n",
        "            t_logits_slct = teacher_outputs\n",
        "            s_logits_slct = student_outputs\n",
        "        return F.mse_loss(s_logits_slct, t_logits_slct)\n",
        "\n",
        "    def calc_ce_loss(self, mask, s_logits, t_logits):\n",
        "        if mask is not None:\n",
        "            # mask has False at padding_idx\n",
        "            sel_mask = mask[:, :, None].expand_as(s_logits)\n",
        "            s_logits_slct = torch.masked_select(\n",
        "                s_logits, sel_mask\n",
        "            )  # (bs * seq_length * voc_size) modulo the 1s in mask\n",
        "            t_logits_slct = torch.masked_select(\n",
        "                t_logits, sel_mask\n",
        "            )  # (bs * seq_length * voc_size) modulo the 1s in mask\n",
        "        else:\n",
        "            t_logits_slct = t_logits\n",
        "            s_logits_slct = s_logits  # (bs * seq_length * voc_size) modulo the 1s in mask\n",
        "        s_logits_slct = s_logits_slct.view(-1, s_logits.size(-1))  # (bs * seq_length, voc_size) modulo the 1s in mask\n",
        "        t_logits_slct = t_logits_slct.view(-1, s_logits.size(-1))  # (bs * seq_length, voc_size) modulo the 1s in mask\n",
        "        assert t_logits_slct.size() == s_logits_slct.size()\n",
        "        loss_ce = (\n",
        "            self.ce_loss_fct(\n",
        "                F.log_softmax(s_logits_slct / self.temperature, dim=-1),\n",
        "                F.softmax(t_logits_slct / self.temperature, dim=-1),\n",
        "            )\n",
        "            * (self.temperature) ** 2\n",
        "        )\n",
        "        return loss_ce, s_logits_slct, t_logits_slct\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
        "        model = self.model\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": self.hparams.weight_decay,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
        "        self.opt = optimizer\n",
        "        return [optimizer]\n",
        "\n",
        "    @staticmethod\n",
        "    def add_model_specific_args(parser, root_dir):\n",
        "        SummarizationModule.add_model_specific_args(parser, root_dir)\n",
        "        parser.add_argument(\"--teacher\", default=\"facebook/bart-large-cnn\", type=str)\n",
        "        parser.add_argument(\"--alpha_ce\", default=0.8, type=float)\n",
        "        parser.add_argument(\"--alpha_mlm\", default=0.2, type=float)\n",
        "        # parser.add_argument(\"--alpha_cos\", default=0.0, type=float)\n",
        "        parser.add_argument(\"--alpha_encoder_loss\", default=0.0, type=float)\n",
        "        parser.add_argument(\"--alpha_hid\", default=0.0, type=float, required=False)\n",
        "        parser.add_argument(\"--student_decoder_layers\", default=12, type=int, required=False)\n",
        "        parser.add_argument(\"--student_encoder_layers\", default=12, type=int, required=False)\n",
        "        parser.add_argument(\"--no_teacher\", action=\"store_true\", default=False)\n",
        "        parser.add_argument(\"--length_penalty\", type=float, default=-1)\n",
        "\n",
        "        return parser\n",
        "\n",
        "    def _step(self, batch):\n",
        "        # assert is_frozen(self.teacher)\n",
        "        pad_token_id = self.tokenizer.pad_token_id\n",
        "        input_ids, src_mask, y = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"decoder_input_ids\"]\n",
        "        decoder_input_ids = y[:, :-1].contiguous()\n",
        "        labels = y[:, 1:].clone()\n",
        "        labels[y[:, 1:] == pad_token_id] = -100\n",
        "        # noinspection PyCallingNonCallable\n",
        "        sloss, slogits, dec_hidden, enc_outputs, enc_hidden_state = self(\n",
        "            input_ids,\n",
        "            attention_mask=src_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            labels=labels,\n",
        "            output_hidden_states=True,\n",
        "            output_attentions=False,\n",
        "        )\n",
        "\n",
        "        def zero_tensor():\n",
        "            return torch.tensor(0.0).type_as(sloss)\n",
        "\n",
        "        loss_encoder, hid_loss_enc, hid_loss_dec = zero_tensor(), zero_tensor(), zero_tensor()\n",
        "        if self.different_encoder:\n",
        "            with torch.no_grad():\n",
        "                teacher_enc_outputs, teacher_enc_hid, _ = self.teacher.model.encoder(\n",
        "                    input_ids, attention_mask=src_mask, output_hidden_states=True\n",
        "                )\n",
        "            if self.hparams.alpha_encoder_loss > 0:\n",
        "                loss_encoder = self.calc_mse_loss(enc_outputs, teacher_enc_outputs, src_mask)\n",
        "\n",
        "            hid_loss_enc = self.calc_hidden_loss(\n",
        "                src_mask, enc_hidden_state, teacher_enc_hid, self.hparams.e_layer_to_copy\n",
        "            )\n",
        "\n",
        "        teacher_enc_outputs = (enc_outputs,)\n",
        "        assert isinstance(teacher_enc_outputs, tuple), type(teacher_enc_outputs)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            tloss, tlogits, tdec_hidden, _ = self.teacher(\n",
        "                input_ids,\n",
        "                attention_mask=src_mask,\n",
        "                encoder_outputs=teacher_enc_outputs,\n",
        "                decoder_input_ids=decoder_input_ids,\n",
        "                lm_labels=labels,\n",
        "                output_hidden_states=True,\n",
        "            )\n",
        "        dec_mask = decoder_input_ids.ne(pad_token_id)\n",
        "        loss_ce, s_logits_slct, t_logits_slct = self.calc_ce_loss(dec_mask, slogits, tlogits)\n",
        "        if self.alpha_hid > 0:\n",
        "            hid_loss_dec = self.calc_hidden_loss(dec_mask, dec_hidden, tdec_hidden, self.hparams.d_layer_to_copy)\n",
        "\n",
        "        blended_loss = (\n",
        "            self.alpha_ce * loss_ce\n",
        "            + self.alpha_mlm * sloss\n",
        "            + self.hparams.alpha_encoder_loss * loss_encoder\n",
        "            + self.hparams.alpha_hid * (hid_loss_enc + hid_loss_dec)\n",
        "        )\n",
        "        return blended_loss, loss_ce, sloss, loss_encoder, hid_loss_enc, hid_loss_dec\n",
        "\n",
        "    def calc_hidden_loss(self, attention_mask, hidden_states, hidden_states_T, matches):\n",
        "        assert not isinstance(\n",
        "            hidden_states, torch.Tensor\n",
        "        ), f\"expected list or tuple for hidden_states, got tensor of shape {hidden_states.shape}\"\n",
        "        assert not isinstance(\n",
        "            hidden_states_T, torch.Tensor\n",
        "        ), f\"expected list or tuple for hidden_states_T, got tensor of shape {hidden_states_T.shape}\"\n",
        "        mask = attention_mask.to(hidden_states[0])\n",
        "        valid_count = mask.sum() * hidden_states[0].size(-1)\n",
        "        hidden_losses = [\n",
        "            (F.mse_loss(hidden_states[i], hidden_states_T[j], reduction=\"none\") * mask.unsqueeze(-1)).sum()\n",
        "            / valid_count\n",
        "            for i, j in enumerate(matches)\n",
        "        ]\n",
        "        return sum(hidden_losses)\n",
        "\n",
        "\n",
        "class T5SummarizationDistiller(BartSummarizationDistiller):\n",
        "    def pre_init(self, hparams):\n",
        "        raise NotImplementedError(\"T5 Distillation does not work yet\")\n",
        "        self.output_dir = Path(hparams.output_dir)\n",
        "        self.output_dir.mkdir(exist_ok=True)\n",
        "        teacher = T5ForConditionalGeneration.from_pretrained(hparams.teacher)\n",
        "        n_layer = hparams.student_decoder_layers\n",
        "        assert n_layer == hparams.student_encoder_layers  # TODO(SS): relax this constraint so that we can do 12-6.\n",
        "        d_layers_to_copy = get_layers_to_copy(n_layer, len(teacher.decoder.block))\n",
        "        e_layers_to_copy: List = get_layers_to_copy(n_layer, len(teacher.encoder.block))\n",
        "        student_updates = {\"num_layers\": n_layer}\n",
        "        hparams.d_layer_to_copy = d_layers_to_copy\n",
        "        hparams.e_layer_to_copy = e_layers_to_copy\n",
        "        kw = teacher.config.to_diff_dict()\n",
        "\n",
        "        kw.update(student_updates)\n",
        "        # Copy weights\n",
        "        student_cfg = T5Config(**kw)\n",
        "        student = T5ForConditionalGeneration(student_cfg)\n",
        "        student, _ = init_student(student, teacher)\n",
        "        self.copy_to_student(d_layers_to_copy, e_layers_to_copy, hparams, student, teacher)\n",
        "        Path(hparams.output_dir).mkdir(exist_ok=True)\n",
        "        task_specific_params = student.config.task_specific_params\n",
        "        if task_specific_params is not None:\n",
        "            student.config.update(task_specific_params.get(\"summarization\", {}))  # TODO: dont hardcode\n",
        "        save_dir = self.output_dir.joinpath(\"student\")\n",
        "        save_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        student.save_pretrained(save_dir)\n",
        "        hparams.model_name_or_path = str(save_dir)\n",
        "        return student, student_cfg, teacher\n",
        "\n",
        "    def freeze_embeds(self):\n",
        "        freeze_params(self.model.shared)\n",
        "        for d in [self.model.encoder, self.model.decoder]:\n",
        "            freeze_params(d.embed_tokens)\n",
        "\n",
        "    def sanity_check_gradients(self):\n",
        "        \"\"\"T5\"\"\"\n",
        "        assert_all_frozen(self.teacher)\n",
        "        assert_all_frozen(self.model.decoder.embed_tokens)\n",
        "        assert_all_frozen(self.model.encoder.embed_tokens)\n",
        "        if self.different_encoder:\n",
        "            assert any_requires_grad(self.model.encoder)\n",
        "        else:\n",
        "            freeze_params(self.model.encoder)\n",
        "            del self.teacher.model.encoder\n",
        "        if self.different_decoder:\n",
        "            assert any_requires_grad(self.model.decoder)\n",
        "        else:\n",
        "            freeze_params(self.model.decoder)  # TODO(SS): very suspicious\n",
        "\n",
        "    def _step(self, batch):\n",
        "        pad_token_id = self.tokenizer.pad_token_id\n",
        "        source_ids, source_mask, y = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"decoder_input_ids\"]\n",
        "        decoder_input_ids = y[:, :-1].contiguous()\n",
        "        labels = y[:, 1:].clone()\n",
        "        labels[y[:, 1:] == pad_token_id] = -100\n",
        "        # noinspection PyCallingNonCallable\n",
        "        dec_mask = decoder_input_ids.ne(pad_token_id)\n",
        "\n",
        "        sloss, slogits, dec_hidden, enc_outputs, enc_hidden_state = self(\n",
        "            source_ids,\n",
        "            attention_mask=source_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            labels=labels,\n",
        "            output_hidden_states=True,\n",
        "            output_attentions=False,\n",
        "            use_cache=False,\n",
        "        )\n",
        "\n",
        "        def zero_tensor():\n",
        "            return torch.tensor(0.0).type_as(sloss)\n",
        "\n",
        "        loss_encoder, hid_loss_enc, hid_loss_dec = zero_tensor(), zero_tensor(), zero_tensor()\n",
        "        if self.different_encoder:\n",
        "            with torch.no_grad():\n",
        "                teacher_enc_outputs, teacher_enc_hid = self.teacher.encoder(\n",
        "                    source_ids, attention_mask=source_mask, output_hidden_states=True, use_cache=False,\n",
        "                )\n",
        "            if self.hparams.alpha_encoder_loss > 0:\n",
        "                loss_encoder = self.calc_mse_loss(enc_outputs, teacher_enc_outputs, source_mask)\n",
        "\n",
        "            hid_loss_enc = self.calc_hidden_loss(\n",
        "                source_mask, enc_hidden_state, teacher_enc_hid, self.hparams.e_layer_to_copy\n",
        "            )\n",
        "\n",
        "        teacher_enc_outputs = (enc_outputs,)\n",
        "        assert isinstance(teacher_enc_outputs, tuple), type(teacher_enc_outputs)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            tloss, tlogits, tdec_hidden, _ = self.teacher(\n",
        "                source_ids,\n",
        "                attention_mask=source_mask,\n",
        "                encoder_outputs=teacher_enc_outputs,\n",
        "                decoder_input_ids=decoder_input_ids,\n",
        "                lm_labels=labels,\n",
        "                output_hidden_states=True,\n",
        "                use_cache=False,\n",
        "            )\n",
        "\n",
        "        loss_ce, s_logits_slct, t_logits_slct = self.calc_ce_loss(dec_mask, slogits, tlogits)\n",
        "        if self.alpha_hid > 0:\n",
        "            hid_loss_dec = self.calc_hidden_loss(dec_mask, dec_hidden, tdec_hidden, self.hparams.d_layer_to_copy)\n",
        "\n",
        "        blended_loss = (\n",
        "            self.alpha_ce * loss_ce\n",
        "            + self.alpha_mlm * sloss\n",
        "            + self.hparams.alpha_encoder_loss * loss_encoder\n",
        "            + self.hparams.alpha_hid * (hid_loss_enc + hid_loss_dec)\n",
        "        )\n",
        "        return blended_loss, loss_ce, sloss, loss_encoder, hid_loss_enc, hid_loss_dec\n",
        "\n",
        "\n",
        "def create_module(args):\n",
        "    t5 = \"t5\" in args.model_name_or_path\n",
        "    if args.no_teacher:\n",
        "        assert not args.enc_only\n",
        "        module_cls = SummarizationModule\n",
        "    elif t5:\n",
        "        module_cls = T5SummarizationDistiller\n",
        "    elif args.enc_only:\n",
        "        raise ValueError(\"Deleted that\")\n",
        "    else:\n",
        "        module_cls = BartSummarizationDistiller\n",
        "    args.setup_cls: str = module_cls.__name__\n",
        "    model = module_cls(args)\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate_checkpoint(ckpt_path: Path, dest_dir=None):\n",
        "    exp_dir = ckpt_path.parent\n",
        "    if dest_dir is None:\n",
        "        dest_dir = exp_dir\n",
        "    clash = list(dest_dir.glob(\"test_generations*\"))\n",
        "    if clash:\n",
        "        print(f\"SKIPPING to avoid overwriting {clash}\")\n",
        "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "    if \"hparams\" in ckpt:\n",
        "        args = argparse.Namespace(**ckpt[\"hparams\"])\n",
        "    else:\n",
        "        args = argparse.Namespace(**pickle_load(exp_dir / \"hparams.pkl\"))\n",
        "    args.resume_from_checkpoint = str(ckpt_path)\n",
        "    args.do_train = False\n",
        "    args.output_dir = str(dest_dir)\n",
        "    args.n_gpu = 1\n",
        "    args.eval_batch_size = 16\n",
        "    Path(args.output_dir).mkdir(exist_ok=True)\n",
        "    model = create_module(args)\n",
        "    trainer: pl.Trainer = generic_train(model, args, early_stopping_callback=False)\n",
        "    trainer.test(model)\n",
        "\n",
        "\n",
        "def get_layers_to_copy(n_to_get, tot):\n",
        "    all_layers = list(range(tot))\n",
        "    if tot == 12:  # Alternating for special cases\n",
        "        layers_to_copy = {  # maps  num layers in student -> which teacher layers to copy\n",
        "            1: [0],\n",
        "            2: [0, 6],\n",
        "            3: [0, 6, 11],\n",
        "            4: [0, 4, 8, 11],\n",
        "            6: [0, 2, 4, 7, 9, 11],\n",
        "            9: [0, 1, 2, 4, 5, 7, 9, 10, 11],\n",
        "            12: all_layers,\n",
        "        }\n",
        "        return layers_to_copy[n_to_get]\n",
        "    else:\n",
        "        return all_layers[:n_to_get]  # TODO: better version on theseus-bart branch\n",
        "\n",
        "\n",
        "def distill_main(args):\n",
        "    Path(args.output_dir).mkdir(exist_ok=True)\n",
        "    if len(os.listdir(args.output_dir)) > 3 and args.do_train:\n",
        "        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n",
        "\n",
        "    model = create_module(args)\n",
        "    return ft_main(args, model=model)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser = pl.Trainer.add_argparse_args(parser)\n",
        "    parser = BartSummarizationDistiller.add_model_specific_args(parser, os.getcwd())\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    distill_main(args)"
      ],
      "metadata": {
        "id": "e3HRmbfEDpj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation Example"
      ],
      "metadata": {
        "id": "dvnrr4_NF1KM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfhwahfoMrJN",
        "outputId": "8d107e49-568a-4ebb-bc65-47930a7e4ae3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model loading ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:418: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:437: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `2.0` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model loaded\n",
            "['PersonX pleases ___ to make xWant']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[' to make a plan']]\n",
            "model loading ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model loaded\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "# from utils import calculate_rouge, use_task_specific_params, calculate_bleu_score, trim_batch\n",
        "\n",
        "\n",
        "def chunks(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i : i + n]\n",
        "\n",
        "\n",
        "class Comet:\n",
        "    def __init__(self, model_path):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        task = \"summarization\"\n",
        "        use_task_specific_params(self.model, task)\n",
        "        self.batch_size = 1\n",
        "        self.decoder_start_token_id = None\n",
        "\n",
        "    def generate(\n",
        "            self,\n",
        "            queries,\n",
        "            decode_method=\"beam\",\n",
        "            num_generate=5,\n",
        "            ):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            examples = queries\n",
        "\n",
        "            decs = []\n",
        "            for batch in list(chunks(examples, self.batch_size)):\n",
        "\n",
        "                batch = self.tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=\"max_length\").to(self.device)\n",
        "                input_ids, attention_mask = trim_batch(**batch, pad_token_id=self.tokenizer.pad_token_id)\n",
        "\n",
        "                summaries = self.model.generate(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    decoder_start_token_id=self.decoder_start_token_id,\n",
        "                    num_beams=num_generate,\n",
        "                    num_return_sequences=num_generate,\n",
        "                    )\n",
        "\n",
        "                dec = self.tokenizer.batch_decode(summaries, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "                decs.append(dec)\n",
        "\n",
        "            return decs\n",
        "\n",
        "\n",
        "all_relations = [\n",
        "    \"AtLocation\",\n",
        "    \"CapableOf\",\n",
        "    \"Causes\",\n",
        "    \"CausesDesire\",\n",
        "    \"CreatedBy\",\n",
        "    \"DefinedAs\",\n",
        "    \"DesireOf\",\n",
        "    \"Desires\",\n",
        "    \"HasA\",\n",
        "    \"HasFirstSubevent\",\n",
        "    \"HasLastSubevent\",\n",
        "    \"HasPainCharacter\",\n",
        "    \"HasPainIntensity\",\n",
        "    \"HasPrerequisite\",\n",
        "    \"HasProperty\",\n",
        "    \"HasSubEvent\",\n",
        "    \"HasSubevent\",\n",
        "    \"HinderedBy\",\n",
        "    \"InheritsFrom\",\n",
        "    \"InstanceOf\",\n",
        "    \"IsA\",\n",
        "    \"LocatedNear\",\n",
        "    \"LocationOfAction\",\n",
        "    \"MadeOf\",\n",
        "    \"MadeUpOf\",\n",
        "    \"MotivatedByGoal\",\n",
        "    \"NotCapableOf\",\n",
        "    \"NotDesires\",\n",
        "    \"NotHasA\",\n",
        "    \"NotHasProperty\",\n",
        "    \"NotIsA\",\n",
        "    \"NotMadeOf\",\n",
        "    \"ObjectUse\",\n",
        "    \"PartOf\",\n",
        "    \"ReceivesAction\",\n",
        "    \"RelatedTo\",\n",
        "    \"SymbolOf\",\n",
        "    \"UsedFor\",\n",
        "    \"isAfter\",\n",
        "    \"isBefore\",\n",
        "    \"isFilledBy\",\n",
        "    \"oEffect\",\n",
        "    \"oReact\",\n",
        "    \"oWant\",\n",
        "    \"xAttr\",\n",
        "    \"xEffect\",\n",
        "    \"xIntent\",\n",
        "    \"xNeed\",\n",
        "    \"xReact\",\n",
        "    \"xReason\",\n",
        "    \"xWant\",\n",
        "    ]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # sample usage (reproducing AAAI)\n",
        "    print(\"model loading ...\")\n",
        "    comet = Comet(\"/content/comet-atomic_2020_BART_aaai\")\n",
        "    comet.model.zero_grad()\n",
        "    print(\"model loaded\")\n",
        "    queries = []\n",
        "    head = \"PersonX pleases ___ to make\"\n",
        "    rel = \"xWant\"\n",
        "    query = \"{} {}\".format(head, rel)\n",
        "    queries.append(query)\n",
        "    print(queries)\n",
        "    results = comet.generate(queries, decode_method=\"greedy\", num_generate=1)\n",
        "    print(results)\n",
        "\n",
        "\n",
        "    # sample usage (reproducing demo)\n",
        "    print(\"model loading ...\")\n",
        "    comet = Comet(\"/content/comet-atomic_2020_BART\")\n",
        "    comet.model.zero_grad()\n",
        "    print(\"model loaded\")\n",
        "    queries = []\n",
        "    head = \"PersonX pleases ___ to make\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rel = \"xIntent\"\n",
        "query = \"{} {} [GEN]\".format(head, rel)\n",
        "queries.append(query)\n",
        "print(queries)\n",
        "results = comet.generate(queries, decode_method=\"beam\", num_generate=1)\n",
        "print(results)\n",
        "\n",
        "rel = \"xEffect\"\n",
        "query = \"{} {} [GEN]\".format(head, rel)\n",
        "queries.append(query)\n",
        "print(queries)\n",
        "results = comet.generate(queries, decode_method=\"beam\", num_generate=1)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXoveDhwOpNZ",
        "outputId": "736d6e2a-9aa2-403e-ac9d-0a70e60649d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PersonX pleases ___ to make xIntent [GEN]']\n",
            "[[' to be nice']]\n",
            "['PersonX pleases ___ to make xIntent [GEN]', 'PersonX pleases ___ to make xEffect [GEN]']\n",
            "[[' to be nice'], [' none']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "upccBeopb8x2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}